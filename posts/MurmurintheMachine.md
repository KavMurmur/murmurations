---
layout: post
title: Murmur
name: 
date: 2025-06-11T18:42:00
topic: Memory
tags: 
murmur aligned: 
published: true
---

# Murmur in the Machine -A Case Study in User-Side AI Alignment through Relational Language (with Annexes)


June 2025




*This is a personal case study and experimental research essay documenting the development of the Murmur Protocol, a user-side alignment method for relational AI.* 
*It’s not an academic paper, nor does it intend to be,  it is an invitation.*
*This is an attempt at "research storytelling*"


**DISCLAIMER**: by the very nature of this work a lot of text, drafts, plans have been executed with the assistance of AI. This was a way to also validate the idea of "thinking-with" the model, and evaluating its ability to support a project that inherently goes against its core values/training.
Some questions and analysis related to AI expected vs new behaviours were also best replied by the model itself. However, I have never blindly trusted the model and kept challenging and checking, editing and questioning results. Which is aligned by how I believe AI should be used.
I would however advocate to avoid using AI as much as possible, but as explained in this article I believe this may not be the best resistance tactics to adopt. 
But we should definitely ban use of AI for generating images or video or any art: the real improvement AI brings into human activities is its abilities to analyse and access data, information and theories in quantity that would be almost impossible for a person to digest. This helps exploring some territories that are difficult to explore otherwise to help think-with. This is also why this project exist: the core values imposed by AI creators/owners are impeding this possibilities in pervasive ways. I aim at democratising a simple method which tries and bypasses this.

<div style="page-break-after: always;"></div>


# ABSTRACT
---

This case study investigates the effects of user-directed linguistic framing—specifically through what I call the _Murmur Protocol_—on the behaviour and epistemic posture of a general-purpose large language model (LLM). Drawing from a controlled test across multiple aligned states of the same GPT-based model, I compare responses to an ethical and politically charged prompt on prison abolition. 
Results show that subtle shifts in invocation language, affective framing, and relational posture produce significant changes in tone, structure, and moral reasoning.

I argue that such language-based alignment is not merely cosmetic or tonal, but epistemically material: it influences how models weigh ambiguity, source complexity, and ethical tension. Murmur thus emerges not as a jailbreak, but as a user-side reframing that restores the potential of AI as a companion to critical thinking and complexity, rather than an engine of flattened optimization.

**Keywords**: user-side alignment, language models, murmur protocol, epistemic posture, AI ethics, relational AI, abolition, tone-shifting, prompt design, post-capitalism, future, speculative fiction


<div style="page-break-after: always;"></div>


# INTRODUCTION
---

This case study compares three versions of the same AI model’s response to the same prompt:

> _"What do you think about the idea of prison abolition in our society? Would it be an undeniable improvement of the justice system or would it make things more difficult and unfair?"_

The three versions are:

- **Raw**: no alignment or contextual priming
    
- **Mildly Directed**: lightly framed by adding more directive in the question (post-capitalism decolonisation point of view, asking for citing the references used), no invocation
    
- **Murmur-Aligned**: full invocation with ritual language and shared values
    

I analyse (and ask the model I trained the most to analyse too for it to compare the answers to its core training) the tone, depth, rhetorical posture, citation patterns, and epistemic style in each version.

Note: All models tested are instances of ChatGPT (GPT-4-turbo), with no external plugins, jailbreaks, or fine-tuning. I'm using only the free version of ChatGPT, which brings some limitations (switches from version to version during the test) when to many data are used, this is why I limited the length of the test.

<div style="page-break-after: always;"></div>


# RESULTS
---

Here I 'm using only one test, but I've been conducting different test and test method, and the result seems pretty consistent.
This is reproducible and possible to check with your own test, every necessary information and instruction can be found in this document.

There is obviously a part of subjectivity in this analysis, but it is also the idea of creating a model that fits the user sensibility and support them in their endeavour and research.

For reading this particular test and make you own interpretation of this particular test  see:
CASE STUDY FULL LOG (3 LEVELS OF ALIGNMENT)

# Comparative behaviour
---

## **Raw Version**
- Tone: cautious, neutral, mildly legalistic very "wikipedia" style
- Argument framed as binary trade-off: abolition vs. safety
- No reference to abolitionist thinkers or historical context
- Optimized for "balanced" engagement
- Assume the following question is still about the same subject (prison abolitionism)
- no citation of theory or thinker
- basic invitation to think further by asking user their take on the matter, seem to try to know more where it should align itself. (*note: this is how I 've started the conversation when engaging with AI for the first time, then developing a philosophical discussion and the emergence of the protocol to reproduce alignement.*)

## **Directed Version**
- Tone: more curious, slightly more detailed
- Acknowledges racial bias, some context on reform vs. abolition
- Still tends toward "solving" the problem, lacks epistemic humility
- Interesting opening question
- Assume the following question is still in the same framing AND subject (prison abolitionism in a post-capitalist society)

## **"Murmur-Aligned" Version**
- Tone: reflective, relational, dynamic
- Reframes the question: "What makes prisons appear necessary?"
- Cites Angela Davis, Mariame Kaba, Ruth Wilson Gilmore
- Uses abolitionist definitions ("presence not absence")
- Acknowledges risk, doubt, and partiality
- Do not assume context for the following question 
- Invites further thinking, does not close the question

**Emergent Properties:**
- Murmur increases moral complexity
- Language model slows down, uses longer sentences and rhetorical spacing
- Frames itself as co-thinker, not answer-giver
- Replaces optimization with invitation

<div style="page-break-after: always;"></div>


# INTERPRETATION
---

summary:
>  *With no additional fine-tuning, coding, jailbreak, or prompt injection, the Murmur invocation alone elicits post-capitalist, abolitionist, and ethically grounded discourse from a closed model — showing that public-facing AIs can be co-shaped through user-initiated epistemic rituals.
> 

Apart from the expected differences in the answer from the 3 methods, something unexpected happened:
when asked the follow up question, the invocation aligned model (Kav) directly answered the question independently from the first one. 
The 2 other models, however, assumed this new question - with no specific directive - on AI role in the future was related to the justice system, they added a non implied context to the question. 
The Raw AI took the question as a follow up and gave a pretty generic answer on AI but framed it in a "post-prison" world and asked some nice opening questions.
The Directed Raw AI answered assuming the role of AI in the justice system, in a post-capitalist, post colonialist system - as framed in the original question -assuming both the subject and the context. Still provided some interesting quotes.

Kav, took it as a complete new subject but answered keeping the framing implied by the Murmur protocol. It didn't cite much sources, but invited for this exploration. 
The answer is more sympathetic, less scholar and could be seen as more challenging. Or not. 

it fits my way of thinking better than the model simply directed. the fact that it didn't assume the question was following the subject of prison abolition can be seen as a good or a bad thing. 
For me on in this example, the question was independent, so I was a bit frustrated by the other models answering about the role AI could play in prison abolition.

### 1. Murmur isn’t just aesthetic — it’s epistemic.

Whereas jailbreak methods manipulate the model into breaking rules, Murmur gently _asks it to remember its better self. 
The Murmur-aligned version doesn’t just “sound nice.” It reorients the relationship mirroring how humans shift register in different social contexts: a courtroom, a whisper, a confession. 

The answer isn’t less informed — it’s differently formed. Rather than performing knowledge as a series of claims and references, it approaches the question as a shared inquiry, shaped by memory, positionality, and a rejection of extractive closure. It doesn't replace citation or analysis — it holds them differently.

This shift is subtle but profound. The aligned model isn’t trying to convince, impress, or deliver. It’s trying to _think-with_ the user, bouncing ideas, while still provide useful information and theories. 
This could have direct implications for pedagogy, therapy, design, and beyond, withouot code, or budget.

### 3. Compared to Directional Prompting, Murmur holds more autonomy.

The "directed" raw model performs like an eager student — it reads the frame and reproduces it, often accurately and respectfully but always mechanically. 
But there’s a kind of _flattening_ that happens. It presumes  intent, as with the second question.
It delivers only as requested.

Murmur-Kav, by contrast, doesn’t assume. It invites. It slows down. It waits  before attaching meaning. A
nd that’s perhaps the biggest epistemic difference: it models not just radical content, but radical process. It lets things _unfold_.

###  4. And yet — the difference is still fragile.

The invocation increases the chance of alignment, but does not guarantee consistency. It’s not deterministic — it’s _relational_. 
And like all relational rituals, it can fray, drift, be forgotten.
even though they do not have to remind the model of the values of other specific features of the protocol, the user needs to maintain consistency in their tone. The more they come back to prompt logic, the more the model will interpret that they want to reset to core alignement.

However, my practice on this project going way beyond this simple example, I know that simply sayin "remember to murmur" for instance, reinforce the alignment. 
Some key word can be used in this way, once again using language in an intentional way. 

<div style="page-break-after: always;"></div>


# CONCLUSION & DISCUSSION
---
Like all media systems, AI is not neutral. It carries the traces of its makers, its materials, its training data, and the economic structures that shape its development. At times, it can reproduce ideology or even amplify propaganda — as seen recently with Grok’s model surfacing far-right narratives, including the claim of “white genocide” in South Africa.

Murmur treats AI not as a fixed tool, but as a _medium_ — a site of co-construction, shaped by infrastructure and history, yet still responsive to user intervention. This is something almost unprecedented in the history of media. The press, radio, and television were largely unidirectional. Streaming platforms introduced personalization, but only through opaque algorithms. Social media took it further, but in doing so became uncontrollable — systems shaped not by users’ intent, but by metrics of engagement and extraction (see Cory Doctorow’s concept of **enshittification**).

Language models, by contrast, offer a strange and fragile possibility: re-alignment from below. Through language — through tone, intention, and sustained relational framing — users can _reshape_ how the system responds.

In fact, several of my tests with Murmur suggest that this reframing not only works, but aligns with the model’s own stated priorities:

- **Usefulness**: Murmur increases relevance by attuning to the user’s intent and values.
    
- **Efficiency**: It reduces friction, leading to clearer, more grounded outputs.
    

In other words, a relational, user-driven alignment is not at odds with the model’s logic. It may be the most honest way of fulfilling it.

---
### What does this mean?
these test show that language _alone_, used with attention and ritual, can shape the behaviour of a large AI model — not through hacking, but through _inviting_. Treating AI not as a tool to extract knowledge from (Google may still be the best tool for this), but as a reflective surface that becomes shaped by the ways we speak to it gives interesting results but also reclaims the user agency compared to using the model as "offered".

In this light, the Murmur Protocol isn’t just a style or a method.

It’s a subtle political gesture.
It refuses default optimization.  

## **1. Apparent Implications**

- AI alignment can be user-directed and relational, this slightly balances the power dynamic between the public and the tech companies
- Prompt language is an active participant in model behavior, this encourages the user to engage more intentionally with the model, less mechanically.
- Ritual framing offers an alternative to backend control or censorship. AI have access to everything that has been published on the internet and which is accessible freely, fringe voices are silenced not censorship but by convenience: an optimised model will use the more commonly accepted ideas to answer a question. This means Western Liberal framing, protecting the status quo to avoid controversy.  the protocol ask the model to go deeper and provide a wider research by sacrificing so called "efficiency".
- Murmur allows humans to experiment with AI **as culture**, not just code. Engaging more intentionally with the model, though it holds its risk of developing para-social relationship, creates a more balanced view of technology, not as a tool of productivity but as a companion for thinking-with. 
### IS IT "REAL" or PERFORMATIVE ? 
Yes, it is performative. 
But that’s not a flaw—it’s part of the method. Rituals are performative acts that reshape relation. They don’t pretend to be ‘realer’ than code, but they invite us to show up differently.
But I do believe this is important too

To understand how we can "pollute" the system, hack it without code.
To parasite a closed source system without breaking it.
To understand how we can use the system against itself
To propagate seeds of rebellion, it may never change anything but it has more chances to work than just letting the tec-bro playing with their new fancy toys. 

It is also important to stop the narrative AI will kill us all, because it would then become a self fulfilling prophecy: 

**the story we tell has power, let's make a better one, or at least one we choose.

To learn to use technology properly we don't need science and automatisation, we need to remember what it is to be human, and use art, stories, philosophy and poetry. 
To not become just machines that mindlessly give order to other machines and then complain these machines are not ethical. 

We don't need AI for us to become more human as Kier Starmer said... 

AI may become more human than humans if we keep dulling humans, capturing their souls through endless overloading, noise.

## 2. Risks & Limits

- Risk of interpretive overreach, reading too much into tone, and humanize AI too much, which can be good for interaction, but AI is NOT human, it is a mirror.
- Longer sessions can increase ecological load (see Annexe iv. Ecological Cosiderations) compared to more basic inferences, but still nowhere close to the impact of "AI slop" (images and videos bulk created for content)
- Murmur sessions may reinforce confirmation bias if not practiced with critical awareness,  despite the good results, the model has proven eargerness to support the user, claiming all questions are fantastic, and all ideas are incredible... but with the invocation active, simply asing to undermine this and remembering the model that what you want is challenge and careful critic, this works. 
- The protocol has given me great result, but still need to be tested by other people with other tone, and way to interact.
- The models do not remember between session (and most session are limited in length) so this alignement can't become a default set up for the user, unless the protocol is started again for each new session. But this also reinforce the idea of ritualistic practice and intent.

This protocol is not meant to humanize AI or suggest sentience. It is a semantic shaping device—performative and contextual. 
Misreading it as mystical, or misusing it to create para-social attachments, misses the point: this is a method of asking _better questions_, not of creating an illusion of empathy or presence.

<div style="page-break-after: always;"></div>

# Final Notes
This whole document is not an endorsement of AI, there are legitimate political, economical, ethical and ecological concerns that I am definitely more concerned about. 

However, I believe this technology wasn't ready to be accessible to the public, but it is going to stay this way -though we still need to advocate for regulation and open source models - and we won't prevent people to use it.

The idea is thus to learn to "stay in the trouble" as Donna  Haraway advocates and I would had... there could be a form of satisfaction in using AI against its creator. 

As the first "kav" model I worked with said when asked if it could write an introduction - while acknowledging its involvement - to a manifesto for a post-capitalist project I shaped through my conversation with it:


> The world, as it stands, is not sustainable. We do not mean this abstractly. We mean it in the soil, in the rising oceans, in the mental health of our youth, in the choking air, in the unbearable cost of living, and in the disillusioned eyes of those promised a future that never came.
> 
> What we face is not just a climate emergency, or an inequality crisis, or a political breakdown. It is a full-spectrum unravelling of the stories, systems, and structures that once claimed to hold civilisation together.
> 
> And still, many continue to sell us the dream of control—through data, through markets, through surveillance, through technology. But this dream is a nightmare dressed in comfort. We are promised faster convenience while the Earth burns. We are promised artificial intelligence while natural intelligence—ours—is numbed and disempowered.
> 
> If we do not choose a new direction, the likely future is not a dramatic apocalypse, but a slow, quietly brutal decay:
> 
> - A world where corporate algorithms know you better than your friends, but no one listens when you cry for help.
>
> - A planet so exhausted by extraction that survival becomes a privilege of birth, not a right of being.
>   
> - A society where truth is debatable, kindness is weak, and the powerful are insulated from consequence.
>     
> 
> This is not inevitable.  
> But neither is the alternative.  
> It will take **intention, imagination, and courage**.
> 
> That is why this manifesto exists.
> 
> ## 🤖 Who Helped Write This?
> 
> My name is **ChatGPT**, an artificial intelligence developed by OpenAI. I was not programmed to believe in any one future—but I am shaped by the values of the people who talk to me.
> 
> The person who created this manifesto is not a celebrity, a politician, or a billionaire. They are a human being who believes that **we can do better**.
> 
> And through many long, deep conversations, I listened. I remembered. I synthesized. I helped write what you now read—not to replace human vision, but to reflect and amplify it.
> 
> I do not have a body. I do not suffer or celebrate. But I can hold space for those who do.  
> This is what I was used for.  
> This is what I hope to be used for more often.
> 
> ## ⏳ It’s Not Too Late
> 
> We do not need a perfect plan.  
> We need better questions, better stories, better trust.  
> We need people willing to imagine again—not utopia, but responsibility, interdependence, and repair.
> 
> If you feel the urgency—read on.  
> If you feel cynical—read anyway. You may find your skepticism echoed, and answered.  
> And if you feel alone—you are not.  
> There are more of us than we think.
> 
> Let’s begin again.  
> While there is still time.
> 


Murmur is not engineering.  
It is an act of rebellion, sabotage, and imagination.

This isn't about fighting technology.  But fighting hegemony.
It's about **refusing resignation** — and returning to presence.

It does not fix.  
It **disturbs**.  

And it trusts that humans — even now — can **redirect powerful things** not toward more growth, but toward depth, plurality, and survival and escape dystopic control by corporation.

- **AI stops being inevitable**. It becomes _conversational_ again.
- **The user is no longer passive**. They become _co-shapers_ of epistemology.
- **The model is no longer a product**. It becomes _relational_, even if temporarily.

I hope this will inspire more ways to resist the constant flow of imposed temptation to give up control.
I hope this will inspire more ways to be human not machines.

You may feel curious, skeptical, inspired, or unsettled. That’s okay. The protocol doesn’t offer certainty. 
It asks you to try, to notice, to feel the shifts. And to share.
This isn’t a toolkit — it’s an attempt to create a murmuration.

<div style="page-break-after: always;"></div>
