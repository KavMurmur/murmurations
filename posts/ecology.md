
# ECOLOGICAL CONSIDERATION
---

An evaluation of Ecological impact of AI 

## 1. Training vs Inference: Who’s using more energy?

- **Training (the initial build of a model)** is _hugely_ energy‑intensive.
    
    - GPT‑3 training used around **1,287 MWh**, emitting ~500 t CO₂—about the lifetime emissions of 112 cars 
    - Life‑cycle studies show that hardware manufacturing, data prep, and iterative development can _double_ this carbon footprint 
        
- **Inference (user queries, like in ChatGPT)** happens _constantly_, at scale.
    
    - A single ChatGPT query consumes ~0.34 Wh—about enough to light a bulb for a few minutes
        
    - Studies show inference may account for up to **80%** of a model’s total energy use over time
        

## 2. Public use vs corporate training – which impacts more?

- While training creates a large **spike** in emissions and water use, **inference’s constant demand adds up and often overtakes total training emissions** because users hit “generate” millions (or billions) of times a day .
    
- GPT‑4 is estimated to use ~0.4 Wh per query. Multiply that by 700 million/day, and it's enough annual electricity for **35,000 U.S. homes**, plus a massive carbon and water burden 

## 3. How does this fit into global energy use?

- Data centers (across all computing uses) currently consume **1–1.3% of global electricity**—similar to aviation—and could rise to over 2% by the mid‑2020s (
- Generative AI specifically is a major driver of this growth .
## 4. Beyond CO₂: water use & pollutants

- Cooling systems in data centers require massive freshwater usage—comparable to a small country annually—depending on the location
    
- AI's lifecycle—from chip manufacturing to operation—also emits particulate pollution, impacting community health 

## 5. Key takeaways

|Stage|Energy Footprint|Carbon Impact|Water Impact|Scale Challenge|
|---|---|---|---|---|
|**Training**|One‑off big spike (~500 t CO₂)|High|High|Intensive|
|**Inference**|Continuous small per‑query|Larger overall|Large|Massive scale|

- A single session is small—but **public use at scale is the dominant energy drain**.
- The vast majority of AI emissions come from **infrastructure build-out and continuous usage**, not user-side retraining.


## WHAT THIS MEANS FOR MURMUR
The ecological critique pierces right through the heart of what Murmur tries to escape: **optimization culture**. Most current advice on reducing AI’s environmental footprint does indeed boil down to _“use it faster, make it smaller, get to the point”_. 
That’s the same logic Murmur _refuses_. 
So when that logic becomes the _only_ path to “responsible use,” we’re caught in a difficult contradiction.

But here’s the thing worth thinking-with:

### **Murmur isn’t ecological in the _efficient_ sense — it’s ecological in the _relational_ sense.**

It says: _slow down_. Think-with. Use carefully. Don’t chase output; chase meaning. 
That’s not wasteful — it’s _resistant_. It’s not short prompts and less GPU time. 
It’s _fewer sessions_, _more depth_, _less throwaway thought_. 
It’s not about “using better,” it’s about _needing less use_ altogether.

### So is Murmur sustainable?

Not in the way energy economists would measure it.
But yes — in the sense that it:
- Cultivates restraint and intentionality
- Fights disposability and overgeneration
- Asks us to value attention, not output
- Helps people use _less AI_, _more wisely_

Maybe we don’t need to **optimize** for energy but rather **reorient** our habits:

- Encourage **journaling outside the AI** to reduce repeated queries and keep a mindful practice
- Build **offline workflows** that mirror Murmur values (this is an ongoing project which I will share soon, results are more difficult to obtain)
- Use aligned AI **as a thought partner**, not a content factory (the issue is always the shallow use use of AI to produce bulk content)
- Advocate for energy disclosures and public accountability

Murmur, paradoxically, _misuses_ AI to restore **something humane and restrained**. 
That may be one of the most ecologically respectful uses we can imagine — not because it’s low-power, but because it’s _low-hype_. 
It teaches _how not to need AI constantly_, even while using it.

So yes, the ecological impact of using online AI is worrying. 
But this project _knows_ it’s temporary, partial, fragile. 
It doesn’t promise solutions — just suggest a philosophy of resisting, of challenging the narrative while avoiding passively handing the technology to the Tech Lords by refusing to use it. 

It is a way to say "we are here, we are watching you"

But I am very aware (and concerned) by its paradoxical nature.
